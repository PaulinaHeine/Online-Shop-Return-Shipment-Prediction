{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Wr0pfaE4fhw2",
        "tgijNUMyfnLl",
        "qBCEnUtqhkFL",
        "P-H4Z6cjgp_e",
        "TLAHNKCPhDVO",
        "9yr5V686hfH3",
        "kfQMdYZZ6ZoG",
        "Bhg4S9bl6GBw",
        "o2LKPMhufqSn",
        "LUwQPaRxiF1d",
        "-yM_1yH4iSP-",
        "ECEMYrvUAtnk",
        "IlDznb5oinCd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulinaHeine/Online-Shop-Return-Shipment-Prediction/blob/main/Online_Shop_Return_Shipment_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YpoNzqfV8ntw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Import Dataset and packages***"
      ],
      "metadata": {
        "id": "Wr0pfaE4fhw2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGrJiD6cfg-J",
        "outputId": "a0f6a88e-eb28-43aa-f5ce-9e2e4409e6db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/My Drive/training.csv')\n",
        "df_test = pd.read_csv('/content/drive/My Drive/test.csv')\n",
        "df_sample = pd.read_csv('/content/drive/My Drive/sample_prediction.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Feature Engineering & Preprocessing***"
      ],
      "metadata": {
        "id": "tgijNUMyfnLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, I will add my engineered features. I will add them first on the train and then on the test set so that these data frames have the same form in order to fit a model on them and predict the probabilities of the test data frame."
      ],
      "metadata": {
        "id": "QkjV_Am0itdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 1 Age"
      ],
      "metadata": {
        "id": "qBCEnUtqhkFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This feature shows the age of the customers. Perhaps, age influences the probability of sending back an item. Older people might be overwhelmed by the technical way of sending back items or they might think longer about what they order so the chance of sending it back is lower, while the younger generation is used to deliver services a lot and they grew up with these techniques so the send back more often."
      ],
      "metadata": {
        "id": "6aMOtQHS95oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "zBSL64Kc-gch"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "#replace impossible date of births with nan\n",
        "df_train[\"dateOfBirth\"]=df_train[\"dateOfBirth\"].replace([\"1655-04-19\"], np.nan)\n",
        "df_train[\"dateOfBirth\"]=df_train[\"dateOfBirth\"].replace([\"?\"], np.nan)\n",
        "\n",
        "#add age\n",
        "now = pd.Timestamp('now')\n",
        "df_train['dateOfBirth'] = pd.to_datetime(df_train['dateOfBirth'], format='%Y-%m-%d')    \n",
        "df_train['dateOfBirth'] = df_train['dateOfBirth'].where(df_train['dateOfBirth'] < now, df_train['dateOfBirth'] -  np.timedelta64(100, 'Y'))   #\n",
        "df_train['age'] = (now - df_train['dateOfBirth']).astype('<m8[Y]') \n",
        "\n",
        "#Fill nans with the mean age \n",
        "df_train.age = df_train.age.fillna(df_train[\"age\"].mean())\n"
      ],
      "metadata": {
        "id": "X5VO6XzwgQne"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "#replace impossible date of births with nan\n",
        "df_test[\"dateOfBirth\"]=df_test[\"dateOfBirth\"].replace([\"?\"], np.nan)\n",
        "\n",
        "#add age\n",
        "now = pd.Timestamp('now')\n",
        "df_test['dateOfBirth'] = pd.to_datetime(df_test['dateOfBirth'], format='%Y-%m-%d')    \n",
        "df_test['dateOfBirth'] = df_test['dateOfBirth'].where(df_test['dateOfBirth'] < now, df_test['dateOfBirth'] -  np.timedelta64(100, 'Y'))  \n",
        "df_test['age'] = (now - df_test['dateOfBirth']).astype('<m8[Y]')    \n",
        "\n",
        "#Fill nans with the mean age \n",
        "df_test.age = df_test.age.fillna(df_test[\"age\"].mean())"
      ],
      "metadata": {
        "id": "cmJzvFcYgVMN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 2 differece of mean price per itemID and real price"
      ],
      "metadata": {
        "id": "P-H4Z6cjgp_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps it is helpful to know, how much an item costs on average based on their itemID. If we look at the difference between the mean price and the real price, we know if a product seems to be expensive or cheap, which might influence the probability of returning an item, because an expensive item needs to convince a lot more because the threshold of keeping it is higher. \n",
        "It is clear, that the price influences first the order decision, but in our dataset, we have only ordered products so if a product is cheaper than the expected price, customers will accept minor quality issues whereas items that are more expensive, need to be nearly perfect. So the decision to keep the product is made when the quality of the product and the fit can be checked."
      ],
      "metadata": {
        "id": "QcNnollU-6G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "#add mean price\n",
        "\n",
        "df = df_train.groupby(by=\"itemID\").mean()[\"price\"]\n",
        "df_train = pd.merge(df_train,df,on='itemID',how=\"right\",suffixes=[\"_plain\",\"_meaniID\"])\n",
        "#calculate the difference\n",
        "df_train[\"diff_rrpID_price\"] = df_train[\"price_plain\"]-df_train[\"price_meaniID\"]\n"
      ],
      "metadata": {
        "id": "IrUq17-WgkWv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "\n",
        "#add mean price\n",
        "df = df_test.groupby(by=\"itemID\").mean()[\"price\"]\n",
        "df_test = pd.merge(df_test,df,on='itemID',how=\"right\",suffixes=[\"_plain\",\"_meaniID\"])\n",
        "#calculte the difference\n",
        "df_test[\"diff_rrpID_price\"] = df_test[\"price_plain\"]-df_test[\"price_meaniID\"]\n"
      ],
      "metadata": {
        "id": "mqiGdgFrg0gw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 3 Target encodet Item ID und returnshipment"
      ],
      "metadata": {
        "id": "TLAHNKCPhDVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Feature aims to show, how likely it is to send an item back, based on the itemID. My thought behind this feature was, that several items might be returned more often than others, based on their ID. For example swimwear, underwear, and particularly cosmetics might be returned a very few times but cloth-like jackets and t-shirts are returned more often. Because of this I wanted the probability to return an item based on the ID in the model."
      ],
      "metadata": {
        "id": "OYOXiY9GCDjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Train\n",
        "\n",
        "df = df_train.groupby(by=\"itemID\").mean()[\"returnShipment\"]\n",
        "df_train = pd.merge(df_train,df,on='itemID',how=\"right\",suffixes=[\"_plain\",\"_prob_iID\"])\n",
        "df_train = df_train.rename(columns = {\"returnShipment_plain_plain\":\"returnShipment_plain\"})\n",
        "df_train = df_train.rename(columns = {\"returnShipment_plain_prob_iID\":\"returnShipment_prob_iID\"})"
      ],
      "metadata": {
        "id": "M5N6NHt1hDH_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "\n",
        "df = df_train.groupby(by=\"itemID\").mean()[\"returnShipment_plain\"]\n",
        "df_test = df_test.merge(df,how=\"left\",on=\"itemID\")\n",
        "df_test = df_test.rename(columns = {\"returnShipment_plain\":\"returnShipment_prob_iID\"})\n",
        "df_test[\"returnShipment_prob_iID\"] =df_test[\"returnShipment_prob_iID\"].fillna(df_test[\"returnShipment_prob_iID\"].mean())"
      ],
      "metadata": {
        "id": "OQjf_VM4g83R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 4 Target encoding manufacturerID"
      ],
      "metadata": {
        "id": "9yr5V686hfH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I thought, that different manufacturers produce on different levels of quality, so the fast fashion manufacture cloth might be returned more often because of quality issues, than high-quality cloth with no issues."
      ],
      "metadata": {
        "id": "sf80opGpDgzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "df = df_train.groupby(by=\"manufacturerID\").mean()[\"returnShipment_plain\"]\n",
        "\n",
        "df_train = pd.merge(df_train,df,on='manufacturerID',how=\"right\",suffixes=[\"\",\"_prob_manuf\"])\n",
        "df_train = df_train.rename(columns = {\"returnShipment_plain_prob_manuf\":\"returnShipment_prob_manuf\"})\n"
      ],
      "metadata": {
        "id": "A5FysBn5hwU1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "df = df_train.groupby(by=\"manufacturerID\").mean()[\"returnShipment_plain\"]\n",
        "\n",
        "df_test = df_test.merge(df,on=\"manufacturerID\",how=\"left\")\n",
        "df_test = df_test.rename(columns = {\"returnShipment_plain\":\"returnShipment_prob_manuf\"})"
      ],
      "metadata": {
        "id": "xFx-o0XahyAu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 5 targetencoding color"
      ],
      "metadata": {
        "id": "kfQMdYZZ6ZoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several colors are neutral (like grey, blue, black) and several colors that are more unusual (like lemon, leopard, bronze). Neutral colors may be more likely to keep because they suit nearly everyone whereas extreme colors, need to fit the customer. Maybe on the website, the lemon-colored jacket looked nice, but in real life, it fits horrible to the customer. With a black jacket this might not happen, they can be other issues that lead to returning the jacket, but the color is not likely to be the reason. So I calculate the probability of returning an item based on the colors of the training dataset and transmit these probabilities on the test set."
      ],
      "metadata": {
        "id": "l_sJxdrdTQ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "\n",
        "# change missspelled color\n",
        "df_train[\"color\"]=df_train[\"color\"].replace([\"brwon\"], [\"brown\"])\n",
        "\n",
        "# targetencode color\n",
        "df=df_train.groupby(by=\"color\").mean()[\"returnShipment_plain\"]\n",
        "\n",
        "df_train = df_train.merge(df,on='color',how=\"left\",suffixes=[\"\",\"_prob_col\"])\n",
        "df_train = df_train.rename(columns = {\"returnShipment_plain_prob_col\":\"returnShipment_prob_col\"})\n"
      ],
      "metadata": {
        "id": "R2Yqui9o6aKD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "\n",
        "# change missspelled color\n",
        "df_test[\"color\"]=df_test[\"color\"].replace([\"brwon\"], [\"brown\"])\n",
        "\n",
        "# targetencode color\n",
        "df=df_train.groupby(by=\"color\").mean()[\"returnShipment_plain\"]\n",
        "\n",
        "df_test = df_test.merge(df,on=\"color\",how=\"left\")\n",
        "df_test = df_test.rename(columns = {\"returnShipment_plain\":\"returnShipment_prob_col\"})\n"
      ],
      "metadata": {
        "id": "QXyMXA5x6ai1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 6 size targetencodet"
      ],
      "metadata": {
        "id": "Bhg4S9bl6GBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The column size contains sizes in many units, there are children sizes, shoe sizes, EU sizes, and more. First, I wanted to categorize the sizes in the different units, but this may lead to mistakes because there is for example a shoe size 39 and an EU cloth 39. I think, that maybe for one itemID, there are different possible sizes. So if shoes are ItemID 1, there could be a 39 or a 39+ and it would mean the same, it differs because of the person who made these entries. So I tried to just group by the sizes so that we would know, which sizes are more likely to be returned independently by the itemID."
      ],
      "metadata": {
        "id": "5rieZaR6Sw1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "\n",
        "df = df_train.groupby(by=\"size\").mean()[\"returnShipment_plain\"]\n",
        "\n",
        "df_train = pd.merge(df_train,df,on='size',how=\"right\",suffixes=[\"\",\"_prob_size\"])\n",
        "df_train = df_train.rename(columns = {\"returnShipment_plain_prob_size\":\"returnShipment_prob_size\"})\n"
      ],
      "metadata": {
        "id": "1BONZwqR6b3V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "\n",
        "df = df_train.groupby(by=\"size\").mean()[\"returnShipment_plain\"]\n",
        "\n",
        "df_test = df_test.merge(df,on=\"size\",how=\"left\")\n",
        "df_test = df_test.rename(columns = {\"returnShipment_plain\":\"returnShipment_prob_size\"})\n",
        "\n",
        "df_test[\"returnShipment_prob_size\"] = df_test[\"returnShipment_prob_size\"].fillna(df_test[\"returnShipment_prob_size\"].mean())\n"
      ],
      "metadata": {
        "id": "K8OjmZob6cDn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 7 Delivery Days "
      ],
      "metadata": {
        "id": "o2LKPMhufqSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even if it is more a kind of preprocessing, the deviverydays may have a huge impact on the decision, whether an item is returned. If it takes too long, the customer may have ordered a replacement for the item or he is just unhappy because of the long delivery Duration and does not want to keep it. But if it arrives a few days after ordering, the customer gets the item in time and is hopefully not miserable about the long duration."
      ],
      "metadata": {
        "id": "5lFvo9qBffW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train\n",
        "\n",
        "df_train[\"DeliveryDuration\"] = df_train[\"DeliveryDuration\"].str.replace('days', '')\n",
        "df_train['DeliveryDuration'] = df_train['DeliveryDuration'].astype(float)\n",
        "df_train[\"DeliveryDuration\"] = df_train[\"DeliveryDuration\"].fillna(df_train[\"DeliveryDuration\"].mean())\n",
        "\n"
      ],
      "metadata": {
        "id": "CQxFgwrNfm8c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "\n",
        "df_test[\"DeliveryDuration\"] = df_test[\"DeliveryDuration\"].str.replace('days', '')\n",
        "df_test['DeliveryDuration'] = df_test['DeliveryDuration'].astype(float)\n",
        "df_test.DeliveryDuration = df_test.DeliveryDuration.fillna(df_test[\"DeliveryDuration\"].mean())\n"
      ],
      "metadata": {
        "id": "a9NA5Csbf866"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature 8 Customer ID"
      ],
      "metadata": {
        "id": "0VQfGcyav2xN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJTmpup8v1m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Prepare the data***"
      ],
      "metadata": {
        "id": "LUwQPaRxiF1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I won't standardize or normalize the data, because I am going to use the GradientBoosting classifier, this does not use any distance measurement so standardization or normalization is not necessary. It may be easier to interpret the data, but because this homework sets the focus on the prediction, I try to keep it as simple as possible."
      ],
      "metadata": {
        "id": "g7CBpVKs-ZsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I am going to sort the test data frame because the adding of features messed it up. I need it ordered by the Id because I will drop the Id ,it should not be a feature since it should not influence the probability of a returnshipment. I am going to add the Id after I made the predictions, so I need the correct order.\n",
        "\n",
        "\n",
        "Also, I will drop some columns of the train and the test data frame. I will drop size, color, itemID and mufacturerID, because these are not encoded, categorical data. I drop the date of birth because I already have the age in the model. I drop state and salutation because I don't think, that the salutation or the home of a person influences, how likely it is to return an item. I also dropped price_meanID, because only the difference to the real price might have an influence on the probability of a return shipment.\n",
        "\n"
      ],
      "metadata": {
        "id": "Xi8Av0l9irWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = df_test.sort_values(by=\"Id\")"
      ],
      "metadata": {
        "id": "ggpj22Yp4muK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop([ \"size\",\"color\",\"dateOfBirth\",\"state\",\"salutation\",\"price_meaniID\",\"Id\",\"itemID\",\"manufacturerID\"], axis =1)\n",
        "df_test = df_test.drop([ \"size\",\"color\",\"dateOfBirth\",\"state\",\"salutation\",\"price_meaniID\",\"Id\",\"itemID\",\"manufacturerID\"], axis =1)"
      ],
      "metadata": {
        "id": "hWhsymYciJVV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now, the two datasets contain the same columns, apart from the return shipment column, which appears only in the training dataset. Because of that, I am going to split the training dataset into X and y. X now contains the exact same columns as df_test. y contains the outcome of X. "
      ],
      "metadata": {
        "id": "P8xn6rH_lVlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df_train[\"returnShipment_plain\"]\n",
        "X = df_train.drop([\"returnShipment_plain\"], axis =1)"
      ],
      "metadata": {
        "id": "7_EnTPVOiMPK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, I will train the Hyperparameters for my model. In order to do that, I will separate 3% of the train data (12.000 datapoints) and tune the hyperparameters on them. I am going to use the 97% rest of the df_train to fit the model."
      ],
      "metadata": {
        "id": "ij6M3377lp5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_h, X_test_rest, y_train_h, y_test_rest = train_test_split(X,y,test_size=0.97, random_state=42)"
      ],
      "metadata": {
        "id": "VcScgz2TiOIR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Hyperarameter tuning***"
      ],
      "metadata": {
        "id": "-yM_1yH4iSP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I'd like to explain why I use Gradient Boosting for this task. The Gradient Boosting Classifier is very flexible and provides many hyperparameters to tune, so the final score increases. It might tend to overfit the training set depending on which value \"subsample\" and \"learning_rate\" take. But with the default values of these parameters and a 5k fold cross-validation on a subset of this size, the danger of overfitting is relatively low."
      ],
      "metadata": {
        "id": "JPaG8rysnDzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I am goint to set a grid of values for the hyperparameter and find the best values for them."
      ],
      "metadata": {
        "id": "tnXxZQYC1NrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_f={\n",
        "    'loss': [\"deviance\", \"exponential\"],  \n",
        "    'n_estimators': [75,50],\n",
        "    'random_state':[42], #set to 42, it is not here to tune, but with a fix random_state I was able to compare the scores during my work.\n",
        "    'max_depth':[3,4,2]\n",
        "    }"
      ],
      "metadata": {
        "id": "ZxK6EXbYiVSQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gbc = GradientBoostingClassifier()\n",
        "\n",
        "gbc_cv=GridSearchCV(gbc,param_f,cv=10,scoring=\"roc_auc\",verbose=1)\n",
        "\n",
        "gbc_cv.fit(X_train_h,y_train_h)\n",
        "\n",
        "print(gbc_cv.best_params_,gbc_cv.best_score_)\n",
        "\n",
        "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
        "{'loss': 'exponential', 'max_depth': 3, 'n_estimators': 75, 'random_state': 42} 0.7292578987717302\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "CkfpXBxaiaO8",
        "outputId": "ec70a189-9fbd-48f6-da79-e7e94bc0ee5b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.model_selection import GridSearchCV\\n\\ngbc = GradientBoostingClassifier()\\n\\ngbc_cv=GridSearchCV(gbc,param_f,cv=10,scoring=\"roc_auc\",verbose=1)\\n\\ngbc_cv.fit(X_train_h,y_train_h)\\n\\nprint(gbc_cv.best_params_,gbc_cv.best_score_)\\n\\nFitting 10 folds for each of 12 candidates, totalling 120 fits\\n{\\'loss\\': \\'exponential\\', \\'max_depth\\': 3, \\'n_estimators\\': 75, \\'random_state\\': 42} 0.7292578987717302\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best values are:\n",
        "\n",
        " loss : 'exponential' \n",
        " \n",
        " max_depth: 3\n",
        " \n",
        " n_estimators: 75 \n",
        " \n",
        " random_state: 42}\n",
        " \n",
        " with a roc_auc of: 0.7292578987717302\n",
        "\n",
        "Now I hardcode these values and fit a model with them on the rest of the dataset which was not used for hyperparameter fitting. Before I do that I split the remaining data again, so I will have a very small test set to evaluate. I know, that you are going to do that, but I want an impression of how good the model is."
      ],
      "metadata": {
        "id": "c17y4a4R68jJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Fitting the modell***"
      ],
      "metadata": {
        "id": "ECEMYrvUAtnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#insert the best values\n",
        "gbc = GradientBoostingClassifier(\n",
        "    loss= \"exponential\",\n",
        "    max_depth = 3,\n",
        "    n_estimators = 75,\n",
        "    random_state = 42 )\n",
        "\n",
        "gbc = gbc.fit (X_test_rest, y_test_rest)\n"
      ],
      "metadata": {
        "id": "DRqt-TqzlAEc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the model is fitted and in the following code we can see, which features are imortant for the model."
      ],
      "metadata": {
        "id": "41YtmCMORvLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fn, fi in sorted(zip(X.columns, gbc.feature_importances_), key=lambda xx: xx[1], reverse=True):\n",
        "  print(f\"{fn}: {fi:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VpfHSwP8z6k",
        "outputId": "a97651eb-7fda-498a-8910-97d787007e36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeliveryDuration: 0.523\n",
            "returnShipment_prob_iID: 0.422\n",
            "age: 0.021\n",
            "price_plain: 0.020\n",
            "diff_rrpID_price: 0.010\n",
            "returnShipment_prob_size: 0.002\n",
            "returnShipment_prob_col: 0.001\n",
            "returnShipment_prob_manuf: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Create prediction file***"
      ],
      "metadata": {
        "id": "IlDznb5oinCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gbc.predict_proba(df_test)\n",
        "\n",
        "df_pred = pd.DataFrame(y_pred[:,1], columns = ['prediction'])\n",
        "\n",
        "df_pred[\"Id\"] = range(400000,531170)\n",
        "\n",
        "df_pred = df_pred[['Id','prediction']]\n",
        "\n",
        "#df_pred.to_csv(r'/content/drive/My Drive/HW5_PaulinaHeine.csv',index=False)\n"
      ],
      "metadata": {
        "id": "Z0aNTrcukjei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the prdiction file that I heve uploaded as well."
      ],
      "metadata": {
        "id": "8XdimPdzZJJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred\n"
      ],
      "metadata": {
        "id": "foDVVw4tyNQ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "f46ddc8f-b649-4627-a8f7-f2df95376ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c8175c37-eeee-4247-99f1-39f4f571d63d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>400000</td>\n",
              "      <td>0.347593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>400001</td>\n",
              "      <td>0.484777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>400002</td>\n",
              "      <td>0.630273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>400003</td>\n",
              "      <td>0.614342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>400004</td>\n",
              "      <td>0.000588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131165</th>\n",
              "      <td>531165</td>\n",
              "      <td>0.073278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131166</th>\n",
              "      <td>531166</td>\n",
              "      <td>0.567946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131167</th>\n",
              "      <td>531167</td>\n",
              "      <td>0.277272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131168</th>\n",
              "      <td>531168</td>\n",
              "      <td>0.527310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131169</th>\n",
              "      <td>531169</td>\n",
              "      <td>0.166671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>131170 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8175c37-eeee-4247-99f1-39f4f571d63d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8175c37-eeee-4247-99f1-39f4f571d63d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8175c37-eeee-4247-99f1-39f4f571d63d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            Id  prediction\n",
              "0       400000    0.347593\n",
              "1       400001    0.484777\n",
              "2       400002    0.630273\n",
              "3       400003    0.614342\n",
              "4       400004    0.000588\n",
              "...        ...         ...\n",
              "131165  531165    0.073278\n",
              "131166  531166    0.567946\n",
              "131167  531167    0.277272\n",
              "131168  531168    0.527310\n",
              "131169  531169    0.166671\n",
              "\n",
              "[131170 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}